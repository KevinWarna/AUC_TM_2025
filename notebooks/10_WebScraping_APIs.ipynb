{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlI591iOHria"
   },
   "source": [
    "# Web Scraping and APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqOqiurZHrih"
   },
   "source": [
    ">### Today\n",
    ">\n",
    "> - [Web Scraping](#Web-Scraping)\n",
    ">\n",
    ">\n",
    "> - [Working with APIs](#Working-with-APIs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CySD47aFHrij"
   },
   "source": [
    "## Web Scraping\n",
    "\n",
    "**Web Scraping** is a technique for the extraction of information from websites by transforming unstructured data (HTML pages) into structured data (databases or spreadsheets). \n",
    "\n",
    "Even if scraping can be manually performed by a user, it is usually implemented using a **web crawler** (i.e. it is usually implemented as an automatic process). For larger scale scraping see, e.g., [Scrapy](https://scrapy.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxBk3xajHrik"
   },
   "source": [
    "The process is an alternative to using already available **API**s (Application Programming Interface), such as those provided by all the major platforms, like *Facebook*, *Google* and *Twitter*. **More below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKSIkJjTHril"
   },
   "source": [
    "### Basics of HTML\n",
    "\n",
    "The **HyperText Markup Language (HTML)** is the standard **descriptive markup** language for web pages.\n",
    "\n",
    "\n",
    "- **Markup** language: a human-readable, explicit system for annotating the content of a document.\n",
    "\n",
    "\n",
    "- **Descriptive** markup languages (e.g. HTML, XML) are used to annotate the structure of a document, as opposed to **procedural** markup languages (e.g. TEX, Postscript), whose main goal is to describe how a document should be processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEpmuO6cHrim"
   },
   "source": [
    "HTML provides a means to annotate the <strong>structural</strong> elements of documents like (different kinds of) headings, paragraphs, lists, links, images, quotes, tables and so forth. Similarly, even if with fewer options, does Markdown (which we are <em>using</em> *here*, check the code!).\n",
    "\n",
    "HTML tags **do not mark the logical structure** of a document, but only its format (e.g. *this is a table*, *this is a h3-type heading*...). It is up to the browser to then use HTML (plus other information, such as *Cascading Style Sheets*), to render a webpage appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIkILt3JHrin"
   },
   "source": [
    "HTML markup relies on a **fixed inventory of tags**, written by using angle brackets. Some tags, e.g. `<p>...</p>`, surround the marked text, and may include subelements. Other tags, e.g. `<br>` or `<img>` introduce content directly.\n",
    "\n",
    "The following is an example of a web page:\n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "    <title>The Adventures of Pinocchio</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h2>Carlo Collodi</h2>\n",
    "    <h1>The Adventures of Pinocchio</h1>\n",
    "    <hr>\n",
    "    <h4>CHAPTER 1</h4>\n",
    "    <br>\n",
    "    <p><i>How it happened that Mastro Cherry, carpenter, found a piece of wood that wept and laughed like a child</i></p>\n",
    "    <br>\n",
    "    <p>Centuries ago there lived--</p>\n",
    "    <p>\"A king!\" my little readers will say immediately.</p>\n",
    "  </body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyrnFEaWHrio"
   },
   "source": [
    "#### DOM (Document Object Model)\n",
    "\n",
    "- *The Document Object Model is a platform- and language-neutral interface that will allow programs and scripts to dynamically access and update the content, structure and style of documents. The document can be further processed and the results of that processing can be incorporated back into the presented page* ([Word Wide Web Consortium](https://www.w3.org/DOM/))\n",
    "\n",
    "\n",
    "- The DOM treats HTML, XHTML, or XML document as a tree structure, in which each node is an object representing a part of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWBjIJuzHriq"
   },
   "source": [
    "![alt text](https://github.com/bloemj/AUC_TMCI_2022/blob/main/notebooks/images/treeStructure.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asiBWMo-Hrir"
   },
   "source": [
    "- It is a standard, on which most modern browsers rely: usually browsers work by parsing an HTML documents into a DOM and later rendering the DOM structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtMUjj9BHrir"
   },
   "source": [
    "### Scraping Web Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHl2cX-xHris"
   },
   "source": [
    ">The following notes are roughly based on the **Chapters 1-3** of: Mitchell, R. (2015). [Web Scraping with Python](http://shop.oreilly.com/product/0636920034391.do), O'Reilly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_mJJrCqHris"
   },
   "source": [
    "#### Modules and Packages Required for Web Scraping\n",
    "\n",
    "**BeautifulSoup**: this library defines [classes and functions](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to pull data (e.g. table, lists, paragraphs) out of HTML and XML files. It provides idiomatic ways of navigating, searching, and modifying the parse tree.\n",
    "\n",
    "\n",
    "**lxml**: to function, BeautifulSoup relies on external HTML-XML parsers. Many options are available, among which the html5lib's and the Python's built-in parsers. We'll rely on the [lxml](http://lxml.de/)'s parser, due to its high performance, reliability and flexibility.\n",
    "\n",
    "\n",
    "**Urllib**: BeautifulSoup does not fetch the web page for us. To do this, we'll rely on the [Urllib](https://docs.python.org/3.7/library/urllib.html#module-urllib) module available in the Python Standard Library, that implements classes and functions which help in opening URLs (authentication, redirections, cookies and so on). We will see another option, **requests**, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MbMIfDTwHrit"
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYz8pUX2Hriu"
   },
   "source": [
    "#### Retrieve and Parse an HTML page\n",
    "\n",
    "`urllib.request.urlopen()` allows us to retrieve our target HTML page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bxaBa7VwHriu"
   },
   "outputs": [],
   "source": [
    "html = urlopen(\"http://www.pythonscraping.com/pages/page1.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yB9o6-LHriv"
   },
   "source": [
    "What if the page doesn't exist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3Omrt03bHriv",
    "outputId": "1070cdcb-b99c-4d17-bc83-29cc531cf7f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: Not Found\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    html = urlopen(\"http://www.pythonscraping.com/pages/page.html\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tZpvksGHriw"
   },
   "source": [
    "Well, let's handle this properly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLULFOn_Hriw"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    html = urlopen(\"http://www.pythonscraping.com/pages/page.html\")\n",
    "except urllib.request.URLError as e:\n",
    "    pass # code your plan B here\n",
    "except urllib.request.URLError as e:\n",
    "    raise # raise any other exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfRypK0_Hrix"
   },
   "source": [
    "We use `BeautifulSoup()` in conjunction with `lxml` to parse out `html` page and store it in the Beautiful Soup format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WjO2PDIVHrix",
    "outputId": "1a9c523f-e553-4786-b38a-5d926b910184"
   },
   "outputs": [],
   "source": [
    "# you might need to to the following:\n",
    "#!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UAon0t4nHrix"
   },
   "outputs": [],
   "source": [
    "html = urlopen(\"http://www.pythonscraping.com/pages/page1.html\")\n",
    "soup_page1 = BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ySn_r5JCHriy"
   },
   "outputs": [],
   "source": [
    "#Let's scrape another couple of pages we'll need in our examples\n",
    "soup_page3 = BeautifulSoup(urlopen(\"http://www.pythonscraping.com/pages/page3.html\"), \"lxml\")\n",
    "soup_wap = BeautifulSoup(urlopen(\"http://www.pythonscraping.com/pages/warandpeace.html\"), \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62ASB2gkHriy"
   },
   "source": [
    "#### Let's look at the nested structure of the page\n",
    "\n",
    "The `prettify()` method allows us to have a look at the structure of the HTML page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UgDAYnFLHriz",
    "outputId": "911b5faa-3943-40f9-ae73-a2eb79a02a25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "<title>A Useful Page</title>\n",
      "</head>\n",
      "<body>\n",
      "<h1>An Interesting Title</h1>\n",
      "<div>\n",
      "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup_page1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HYDyRPsEHriz",
    "outputId": "b6fa2c48-fe33-45f0-d260-3d25c0afedde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   A Useful Page\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <h1>\n",
      "   An Interesting Title\n",
      "  </h1>\n",
      "  <div>\n",
      "   Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup_page1.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aN3feNYdHriz"
   },
   "source": [
    "#### Let's play with a HTML tag\n",
    "\n",
    "The notation `soup.<tag>` allows us to retrieve the content marked by a tag (opening and closing tags included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUEqDdZiHri0",
    "outputId": "3299f2c1-8427-4877-cd06-13f8bfb0eb5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div>\n",
       "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that the first \"<div>\" tag is nested two layers deep (html → body → div).\n",
    "soup_page1.div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8NELfq_Hri0"
   },
   "source": [
    "If the text is the only thing you're interested into, well, the `soup.<tag>.string` method comes in handy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m9_PpYJSHri0",
    "outputId": "b67df1ea-5650-45db-f4b2-9318d2a50ef9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_page1.div.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmmxiipgHri0"
   },
   "source": [
    "The HTML markup generated by Beautiful Soup can be modified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahDvSkXAHri1"
   },
   "outputs": [],
   "source": [
    "# let's change the content of our div\n",
    "soup_page1.div.string = \"this content has been changed\"\n",
    "# let's change the name of the tag\n",
    "soup_page1.div.name = \"new_div\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PMxoveYzHri1",
    "outputId": "cf4f3cf5-622d-4408-fb46-bc3fc18754ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   A Useful Page\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <h1>\n",
      "   An Interesting Title\n",
      "  </h1>\n",
      "  <new_div>\n",
      "   this content has been changed\n",
      "  </new_div>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup_page1.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3TJ_tJXHri1"
   },
   "source": [
    "In its simplest use, the `find()` method is an alternative to the `soup.<tag>` notation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o0QXtiEZHri1",
    "outputId": "95a4efbd-a833-40b8-eb12-de53554f5c92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<new_div>this content has been changed</new_div>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_page1.find(\"new_div\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2t-Z5Z7rHri2",
    "outputId": "c4418f0f-c862-4252-ac8b-bff7e53bb94e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<new_div>this content has been changed</new_div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_page1.new_div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3l7rjNcHri2"
   },
   "source": [
    "...but this function allows for the searching of nodes by exploiting cues in the markup, such as a given **class attribute** value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyI-XrCfHri7",
    "outputId": "aabac626-2b7e-442a-8564-f3d091df9f64"
   },
   "outputs": [],
   "source": [
    "print(soup_wap.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_O6BjVEKHri8",
    "outputId": "1d31e056-7eb5-4eae-d8c6-67861bf5b7ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<span class=\"green\">Anna\n",
       "Pavlovna Scherer</span>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_wap.find(\"span\", attrs = {\"class\":\"green\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsN_kuIuHri8"
   },
   "source": [
    "The values of an attribute for a given tag instance can be retrieved by using the `get(\"ATTRIBUTE\")` method. For instance, if we want to retrieve the URL of an image we can extract the `src` value from the corresponding `<img>` tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LR7ZUwaVHri8",
    "outputId": "721fef67-87bc-437d-bc31-a2725e6db4a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../img/gifts/logo.jpg'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_page3.img.get(\"src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBUxkn1_Hri8"
   },
   "source": [
    "If we want to know all the attibutes associated with a given tag, the `attrs` method is convenient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NoVZ3_3tHri9",
    "outputId": "553fa68c-c151-4d19-8c11-02b4ca85e2ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src': '../img/gifts/logo.jpg', 'style': 'float:left;'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_page3.img.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45UXz23iHri9",
    "outputId": "de47b0d8-9f44-4e71-e67f-77a96423b50f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../img/gifts/logo.jpg'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by returning a dictionary, it is easy to see how \"attrs\" can be used as an alternative to \"get()\"\n",
    "soup_page3.img.attrs[\"src\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "REmaBXD1Hri9",
    "outputId": "8897d6db-c21f-4fc4-b8a3-537026410e21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../img/gifts/logo.jpg'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you fancy another way to do the same thing...\n",
    "soup_page3.img[\"src\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-p7FQLxHri9"
   },
   "source": [
    "#### Dealing with multiple HTML tags at once\n",
    "\n",
    "When the same tag is used multiple time in the same page, however, both the `soup.<tag>` notation and the `find()` method allow you to access **only one instance** (i.e. the first):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04Tj4KLHHri9",
    "outputId": "0153f67b-f10e-45f2-87e9-c68f7588a06c"
   },
   "outputs": [],
   "source": [
    "print(soup_wap.prettify()[180:1190])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6hA0QVwHri-",
    "outputId": "5dcdc2a6-d969-44f5-b696-66984df1af96"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<span class=\"red\">Well, Prince, so Genoa and Lucca are now just family estates of the\n",
       "Buonapartes. But I warn you, if you don't tell me that this means war,\n",
       "if you still try to defend the infamies and horrors perpetrated by\n",
       "that Antichrist- I really believe he is Antichrist- I will have\n",
       "nothing more to do with you and you are no longer my friend, no longer\n",
       "my 'faithful slave,' as you call yourself! But how do you do? I see\n",
       "I have frightened you- sit down and tell me all the news.</span>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_wap.span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yxkwLhzHri-"
   },
   "source": [
    "In order to extract the **sequence of all the instances of a tag** in a file, we can use the `find_all()` method (previously known as `findAll()` and `findChildren()` in BS 3 and BS 2, respectively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9Ktr2qFHri-",
    "outputId": "4054b687-51fc-4c9e-d3ba-6434535b4052"
   },
   "outputs": [],
   "source": [
    "soup_wap.find_all(\"span\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTaljr-3Hri-"
   },
   "source": [
    "The `find_all()` method as well allows for  the extraction of  all tags by exploiting cues in the markup, such as a given **class attribute** value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MbTk6iPPHri-",
    "outputId": "0b2c561e-aba6-4bd4-bffe-4a359452f497"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"green\">Anna\n",
       " Pavlovna Scherer</span>, <span class=\"green\">Empress Marya\n",
       " Fedorovna</span>, <span class=\"green\">Prince Vasili Kuragin</span>, <span class=\"green\">Anna Pavlovna</span>, <span class=\"green\">St. Petersburg</span>, <span class=\"green\">the prince</span>, <span class=\"green\">Anna Pavlovna</span>, <span class=\"green\">Anna Pavlovna</span>, <span class=\"green\">the prince</span>, <span class=\"green\">the prince</span>, <span class=\"green\">the prince</span>, <span class=\"green\">Prince Vasili</span>, <span class=\"green\">Anna Pavlovna</span>, <span class=\"green\">Anna Pavlovna</span>, <span class=\"green\">the prince</span>, <span class=\"green\">Wintzingerode</span>, <span class=\"green\">King of Prussia</span>, <span class=\"green\">le Vicomte de Mortemart</span>, <span class=\"green\">Montmorencys</span>, <span class=\"green\">Rohans</span>, <span class=\"green\">Abbe Morio</span>, <span class=\"green\">the Emperor</span>, <span class=\"green\">the prince</span>, <span class=\"green\">Prince Vasili</span>, <span class=\"green\">Dowager Empress Marya Fedorovna</span>, <span class=\"green\">the baron</span>, <span class=\"green\">Anna Pavlovna</span>, <span class=\"green\">the Empress</span>, <span class=\"green\">the Empress</span>, <span class=\"green\">Anna Pavlovna's</span>, <span class=\"green\">Her Majesty</span>, <span class=\"green\">Baron\n",
       " Funke</span>, <span class=\"green\">The prince</span>, <span class=\"green\">Anna\n",
       " Pavlovna</span>, <span class=\"green\">the Empress</span>, <span class=\"green\">The prince</span>, <span class=\"green\">Anatole</span>, <span class=\"green\">the prince</span>, <span class=\"green\">The prince</span>, <span class=\"green\">Anna\n",
       " Pavlovna</span>, <span class=\"green\">Anna Pavlovna</span>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_wap.find_all(\"span\",  attrs = {\"class\":\"green\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_oOaAs2Hri-"
   },
   "source": [
    "#### Navigating the tree\n",
    "\n",
    "The DOM trats HTML pages as trees. \n",
    "\n",
    "Beautiful Soup implements several methods that allows you to move in this structure, by starting from a given node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OV9SkEseHri_"
   },
   "source": [
    "![alt text](https://github.com/bloemj/AUC_TMCI_2022/blob/main/notebooks/images/DOM-table.gif?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xO7rrrahHri_"
   },
   "source": [
    "**Going Down**\n",
    "\n",
    "You can iterate over a node's children using the `.children` generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4jYyh2rHri_",
    "outputId": "8b7bcfe2-b470-4983-8adb-6dfde5155dde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <style>\n",
      "   img{\n",
      "\twidth:75px;\n",
      "}\n",
      "table{\n",
      "\twidth:50%;\n",
      "}\n",
      "td{\n",
      "\tmargin:10px;\n",
      "\tpadding:10px;\n",
      "}\n",
      ".wrapper{\n",
      "\twidth:800px;\n",
      "}\n",
      ".excitingNote{\n",
      "\tfont-style:italic;\n",
      "\tfont-weight:bold;\n",
      "}\n",
      "  </style>\n",
      " </head>\n",
      " <body>\n",
      "  <div id=\"wrapper\">\n",
      "   <img src=\"../img/gifts/logo.jpg\" style=\"float:left;\"/>\n",
      "   <h1>\n",
      "    Totally Normal Gifts\n",
      "   </h1>\n",
      "   <div id=\"content\">\n",
      "    Here is a collection of totally normal, totally reasonable gifts that your friends are sure to love! Our collection is\n",
      "hand-curated by well-paid, free-range Tibetan monks.\n",
      "    <p>\n",
      "     We haven't figured out how to make online shopping carts yet, but you can send us a check to:\n",
      "     <br/>\n",
      "     123 Main St.\n",
      "     <br/>\n",
      "     Abuja, Nigeria\n",
      "We will then send your totally amazing gift, pronto! Please include an extra $5.00 for gift wrapping.\n",
      "    </p>\n",
      "   </div>\n",
      "   <table id=\"giftList\">\n",
      "    <tr>\n",
      "     <th>\n",
      "      Item Title\n",
      "     </th>\n",
      "     <th>\n",
      "      Description\n",
      "     </th>\n",
      "     <th>\n",
      "      Cost\n",
      "     </th>\n",
      "     <th>\n",
      "      Image\n",
      "     </th>\n",
      "    </tr>\n",
      "    <tr class=\"gift\" id=\"gift1\">\n",
      "     <td>\n",
      "      Vegetable Basket\n",
      "     </td>\n",
      "     <td>\n",
      "      This vegetable basket is the perfect gift for your health conscious (or overweight) friends!\n",
      "      <span class=\"excitingNote\">\n",
      "       Now with super-colorful bell peppers!\n",
      "      </span>\n",
      "     </td>\n",
      "     <td>\n",
      "      $15.00\n",
      "     </td>\n",
      "     <td>\n",
      "      <img src=\"../img/gifts/img1.jpg\"/>\n",
      "     </td>\n",
      "    </tr>\n",
      "    <tr class=\"gift\" id=\"gift2\">\n",
      "     <td>\n",
      "      Russian Nesting Dolls\n",
      "     </td>\n",
      "     <td>\n",
      "      Hand-painted by trained monkeys, these exquisite dolls are priceless! And by \"priceless,\" we mean \"extremely expensive\"!\n",
      "      <span class=\"excitingNote\">\n",
      "       8 entire dolls per set! Octuple the presents!\n",
      "      </span>\n",
      "     </td>\n",
      "     <td>\n",
      "      $10,000.52\n",
      "     </td>\n",
      "     <td>\n",
      "      <img src=\"../img/gifts/img2.jpg\"/>\n",
      "     </td>\n",
      "    </tr>\n",
      "    <tr class=\"gift\" id=\"gift3\">\n",
      "     <td>\n",
      "      Fish Painting\n",
      "     </td>\n",
      "     <td>\n",
      "      If something seems fishy about this painting, it's because it's a fish!\n",
      "      <span class=\"excitingNote\">\n",
      "       Also hand-painted by trained monkeys!\n",
      "      </span>\n",
      "     </td>\n",
      "     <td>\n",
      "      $10,005.00\n",
      "     </td>\n",
      "     <td>\n",
      "      <img src=\"../img/gifts/img3.jpg\"/>\n",
      "     </td>\n",
      "    </tr>\n",
      "    <tr class=\"gift\" id=\"gift4\">\n",
      "     <td>\n",
      "      Dead Parrot\n",
      "     </td>\n",
      "     <td>\n",
      "      This is an ex-parrot!\n",
      "      <span class=\"excitingNote\">\n",
      "       Or maybe he's only resting?\n",
      "      </span>\n",
      "     </td>\n",
      "     <td>\n",
      "      $0.50\n",
      "     </td>\n",
      "     <td>\n",
      "      <img src=\"../img/gifts/img4.jpg\"/>\n",
      "     </td>\n",
      "    </tr>\n",
      "    <tr class=\"gift\" id=\"gift5\">\n",
      "     <td>\n",
      "      Mystery Box\n",
      "     </td>\n",
      "     <td>\n",
      "      If you love suprises, this mystery box is for you! Do not place on light-colored surfaces. May cause oil staining.\n",
      "      <span class=\"excitingNote\">\n",
      "       Keep your friends guessing!\n",
      "      </span>\n",
      "     </td>\n",
      "     <td>\n",
      "      $1.50\n",
      "     </td>\n",
      "     <td>\n",
      "      <img src=\"../img/gifts/img6.jpg\"/>\n",
      "     </td>\n",
      "    </tr>\n",
      "   </table>\n",
      "   <div id=\"footer\">\n",
      "    © Totally Normal Gifts, Inc.\n",
      "    <br/>\n",
      "    +234 (617) 863-0736\n",
      "   </div>\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup_page3.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ifok1psHri_",
    "outputId": "33c7b39f-74d8-4067-d5c9-b8ae9ee942fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<tr><th>\n",
      "Item Title\n",
      "</th><th>\n",
      "Description\n",
      "</th><th>\n",
      "Cost\n",
      "</th><th>\n",
      "Image\n",
      "</th></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift1\"><td>\n",
      "Vegetable Basket\n",
      "</td><td>\n",
      "This vegetable basket is the perfect gift for your health conscious (or overweight) friends!\n",
      "<span class=\"excitingNote\">Now with super-colorful bell peppers!</span>\n",
      "</td><td>\n",
      "$15.00\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img1.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift2\"><td>\n",
      "Russian Nesting Dolls\n",
      "</td><td>\n",
      "Hand-painted by trained monkeys, these exquisite dolls are priceless! And by \"priceless,\" we mean \"extremely expensive\"! <span class=\"excitingNote\">8 entire dolls per set! Octuple the presents!</span>\n",
      "</td><td>\n",
      "$10,000.52\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img2.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift3\"><td>\n",
      "Fish Painting\n",
      "</td><td>\n",
      "If something seems fishy about this painting, it's because it's a fish! <span class=\"excitingNote\">Also hand-painted by trained monkeys!</span>\n",
      "</td><td>\n",
      "$10,005.00\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img3.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift4\"><td>\n",
      "Dead Parrot\n",
      "</td><td>\n",
      "This is an ex-parrot! <span class=\"excitingNote\">Or maybe he's only resting?</span>\n",
      "</td><td>\n",
      "$0.50\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img4.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift5\"><td>\n",
      "Mystery Box\n",
      "</td><td>\n",
      "If you love suprises, this mystery box is for you! Do not place on light-colored surfaces. May cause oil staining. <span class=\"excitingNote\">Keep your friends guessing!</span>\n",
      "</td><td>\n",
      "$1.50\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img6.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for child in soup_page3.find(\"table\", {\"id\":\"giftList\"}).children:\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuWyJdcaHri_"
   },
   "source": [
    "**Going Up**\n",
    "\n",
    "You can access an element's parent with the `.parent` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PO7JPb3oHrjA",
    "outputId": "a44877fb-2fb7-4db7-c485-f078c8167010",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<td>\n",
      "<img src=\"../img/gifts/img1.jpg\"/>\n",
      "</td>\n"
     ]
    }
   ],
   "source": [
    "print(soup_page3.find(\"img\", {\"src\":\"../img/gifts/img1.jpg\"}).parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpU9S1SSHrjA"
   },
   "source": [
    "You can iterate over all of an element’s parents with `.parents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FD87UJYSHrjA",
    "outputId": "88441382-11fe-4970-9055-a68b00a4abcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "td\n",
      "tr\n",
      "table\n",
      "div\n",
      "body\n",
      "html\n",
      "[document]\n"
     ]
    }
   ],
   "source": [
    "for upper_node in soup_page3.find(\"img\", {\"src\":\"../img/gifts/img1.jpg\"}).parents:\n",
    "    print(upper_node.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ulW1EuOHrjA"
   },
   "source": [
    "You can use `find_parents()` to search for a given parent node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8UmEdHbrHrjA",
    "outputId": "97dce745-7b07-458c-c0b0-28c955d18f51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "div\n"
     ]
    }
   ],
   "source": [
    "for upper_node in soup_page3.find(\"img\", {\"src\":\"../img/gifts/img1.jpg\"}).find_parents(\"div\"):\n",
    "    print(upper_node.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoZNchSRHrjB"
   },
   "source": [
    "**Siblings**\n",
    "\n",
    "You can use `find_previous_siblings()` and `.find_next_siblings()` to navigate between the siblings (i.e. page elements that are on the same level of the parse tree) that precede or come after in the three, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I9YOC3Z5HrjB",
    "outputId": "45d583a0-e53c-4d0a-9678-5ce21e5e4d4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "div\n",
      "h1\n",
      "img\n"
     ]
    }
   ],
   "source": [
    "for sibling in soup_page3.find(\"table\",{\"id\":\"giftList\"}).find_previous_siblings():\n",
    "    print(sibling.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qWu8tAQ9HrjB",
    "outputId": "772f452d-efdd-404b-bba3-7f0e08392457"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "div\n"
     ]
    }
   ],
   "source": [
    "for sibling in soup_page3.find(\"table\",{\"id\":\"giftList\"}).find_next_siblings():\n",
    "    print(sibling.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAIugg9CHrjB"
   },
   "source": [
    "### Web Crawling\n",
    "\n",
    "Web Crawlers are softwares designed to collect pages from the Web. In essence, they recursively implement the following steps: \n",
    "\n",
    "- they start by retrieving the page content for an URL \n",
    "\n",
    "\n",
    "- they then parse it to retrieve other URLs of interest\n",
    "\n",
    "\n",
    "- they then focus on these new URLs, for each of which they repeat the whole process, ad infinitum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awxrS5LdHrjB"
   },
   "source": [
    "For instance, if you want to crawl and **entire site**:\n",
    "\n",
    "- start with a top-level page\n",
    "\n",
    "\n",
    "- parse the page (retrieve the data your application need) and extract all the internal links, by ignoring already visited URLs\n",
    "\n",
    "\n",
    "- for each new link, move to the corresponding page and repeat the previous step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCI9NP3XHrjB"
   },
   "source": [
    "#### A Random walk through Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbN1i0L-HrjB"
   },
   "source": [
    "Let's set our starting page URL, fetch it and parse its HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lpPV8hgXHrjC"
   },
   "outputs": [],
   "source": [
    "starting_page = urlopen(\"https://en.wikipedia.org/wiki/Chris_Cornell\")\n",
    "soup = BeautifulSoup(starting_page, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3ZdmssYHrjC"
   },
   "source": [
    "At this point, it should be easy to extract all the links in the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MZEwAdhzHrjC",
    "outputId": "862d4625-ea39-497f-8705-0f8d39fca5b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"mw-jump-link\" href=\"#bodyContent\">Jump to content</a>\n",
      "<a accesskey=\"z\" href=\"/wiki/Main_Page\" title=\"Visit the main page [z]\"><span>Main page</span></a>\n",
      "<a href=\"/wiki/Wikipedia:Contents\" title=\"Guides to browsing Wikipedia\"><span>Contents</span></a>\n",
      "<a href=\"/wiki/Portal:Current_events\" title=\"Articles related to current events\"><span>Current events</span></a>\n",
      "<a accesskey=\"x\" href=\"/wiki/Special:Random\" title=\"Visit a randomly selected article [x]\"><span>Random article</span></a>\n",
      "<a href=\"/wiki/Wikipedia:About\" title=\"Learn about Wikipedia and how it works\"><span>About Wikipedia</span></a>\n",
      "<a href=\"//en.wikipedia.org/wiki/Wikipedia:Contact_us\" title=\"How to contact Wikipedia\"><span>Contact us</span></a>\n",
      "<a href=\"https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en\" title=\"Support us by donating to the Wikimedia Foundation\"><span>Donate</span></a>\n",
      "<a href=\"/wiki/Help:Contents\" title=\"Guidance on how to use and edit Wikipedia\"><span>Help</span></a>\n",
      "<a href=\"/wiki/Help:Introduction\" title=\"Learn how to edit Wikipedia\"><span>Learn to edit</span></a>\n"
     ]
    }
   ],
   "source": [
    "# links are defined by <a> tag\n",
    "for link in soup.find_all(\"a\")[:10]:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdX-dqgHHrjC"
   },
   "source": [
    "Let's ignore all the \"a\" tags without an \"href\" attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "au2UM41_HrjC",
    "outputId": "4967aee2-2e69-4d74-a990-ede939307ffb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#mw-head\n",
      "#p-search\n",
      "/wiki/File:ChrisCornellTIFFSept2011.jpg\n",
      "/wiki/Seattle,_Washington\n",
      "/wiki/Detroit,_Michigan\n",
      "/wiki/Suicide_by_hanging\n",
      "/wiki/Hollywood_Forever_Cemetery\n",
      "/wiki/Susan_Silver\n",
      "/wiki/Alternative_metal\n",
      "/wiki/Heavy_metal_music\n"
     ]
    }
   ],
   "source": [
    "for link in [tag for tag in soup.find_all(\"a\") if 'href' in tag.attrs][:10]:\n",
    "    print(link.attrs['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4y7ztwfTHrjC"
   },
   "source": [
    "Wikipedia is full of sidebar, footer, and header links that appear on every page, along with links to the category pages, talk pages, and other pages that do not contain different articles:\n",
    "\n",
    "```\n",
    "/wiki/Template_talk:Chris_Cornell\n",
    "```\n",
    "\n",
    "```\n",
    "#cite_note-147\n",
    "```\n",
    "\n",
    "Moreover, we don't want to visit pages outside of Wikipedia:\n",
    "\n",
    "```\n",
    "http://www.chriscornell.com/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElEz5xUaHrjD"
   },
   "source": [
    "Relevant links have three thing in common:\n",
    "\n",
    "- they reside within the `div` with the `id` set to `bodyContent`\n",
    "\n",
    "\n",
    "- the URLs do not contain semicolons\n",
    "\n",
    "\n",
    "- the URLs begin with `/wiki/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhHV2pwIHrjD",
    "outputId": "7c5bac0d-28fe-484b-e015-40e5362892b5"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for link in soup.find(\"div\", {\"id\": \"bodyContent\"}).findAll(\"a\", href=re.compile(\"^(/wiki/)((?!:).)*$\")):\n",
    "    if 'href' in link.attrs:\n",
    "        print(link.attrs['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUff3DQuHrjE"
   },
   "source": [
    "This code returns the list of all the Wikipedia articles linked to our starting page. \n",
    "\n",
    "This is not enough, we want to be recursively repeat this process for all these links. That is, we need a function that takes as input a Wikipedia article URL of the form `/wiki/<Article_Name>` and returns a list of all linked articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDrBZD1MHrjE"
   },
   "outputs": [],
   "source": [
    "def getLinks(articleUrl):\n",
    "    page = urlopen(\"http://en.wikipedia.org\" + articleUrl)\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    links = soup.find(\"div\", {\"id\":\"bodyContent\"}).findAll(\"a\", href=re.compile(\"^(/wiki/)((?!:).)*$\"))\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZSdZQ32HrjE"
   },
   "source": [
    "Let's test our function by calling it in a script that randomly select, for each iteration, a random link and that stops after 10 URLs have been retrieved (or when it bumps into a page without link):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hyZWVquXHrjF",
    "outputId": "e90c8d59-191d-41fc-c3e1-29bd4d28a08d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/wiki/Cannabis\n",
      "/wiki/Nabumetone\n",
      "/wiki/Rofecoxib\n",
      "/wiki/PubMed_Identifier\n",
      "/wiki/PubMed_Identifier\n",
      "/wiki/Digital_object_identifier\n",
      "/wiki/HTTP_proxy\n",
      "/wiki/VPN\n",
      "/wiki/Two-factor_authentication\n",
      "/wiki/Application_security\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "links = getLinks(\"/wiki/Chris_Cornell\")\n",
    "\n",
    "for _ in range(10):\n",
    "    if len(links) > 0:\n",
    "        newArticle = links[random.randint(0, len(links)-1)].attrs[\"href\"]\n",
    "        print(newArticle)\n",
    "        links = getLinks(newArticle)\n",
    "    else:\n",
    "        print(\"no links in this page\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1o1rgN8rHrjF"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCONYolFHrjF"
   },
   "source": [
    "### Exercise 1.\n",
    "\n",
    "Write code to retrieve the official address of the Internationally Ranked Universities in the Netherlands by starting from the following Wikipedia article:\n",
    "\n",
    "https://en.wikipedia.org/wiki/List_of_universities_in_the_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "H71P2DjmHrjF",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifEhOutVHrjF"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7dn7o3EHrjG"
   },
   "source": [
    "## Working with APIs\n",
    "\n",
    "An **Application Programming Interface** is a set of protocols that defines how software programs communicate among eachother. Without APIs, we have to scrape the Web or get the data directly. With APIs, we often can get structured data: it is a much more convenient way to work.\n",
    "\n",
    "APIs are a great option in that they implement extensively tested routines (**high reliability**). However, you should spend time in learning how they work and, in some cases, they don't allow you to access the piece of information you may need (**low flexibility**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "kFJaJWTqHrjG"
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SqI7q3GWHrjG"
   },
   "outputs": [],
   "source": [
    "# Example of a Google search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "QhMeSmLwHrjG"
   },
   "outputs": [],
   "source": [
    "query = \"Tesla\"\n",
    "r = requests.get('https://www.google.com/search', params={'q': query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cGWUiA5SHrjG",
    "outputId": "7002460f-86e1-4f5f-c7a6-572ce249e49f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "dvYwttQLHrjG",
    "outputId": "75261e85-35c6-46c0-fd2c-283e1321f2c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text/html; charset=utf-8\n",
      "utf-8\n",
      "https://consent.google.com/ml?continue=https://www.google.com/search%3Fq%3DTesla&gl=NL&m=0&pc=srp&uxe=none&cm=2&hl=nl&src=1\n"
     ]
    }
   ],
   "source": [
    "print(r.headers['content-type'])\n",
    "print(r.encoding)\n",
    "print(r.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ujUACvHCHrjH",
    "outputId": "35732792-48fa-4d59-bdfe-49545e776ef3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html><html lang=\"nl\"><head><meta charset=\"UTF-8\"><meta content=\"/images/branding/googleg/1x/googleg_standard_color_128dp.png\" itemprop=\"image\"><title>Tesla - Google zoeken</title><script nonce=\"/UfDvGuTENH7v4GHQofzrw==\">(function(){var a=window.performance;window.start=(new Date).getTime();a:{var b=window;if(a){var c=a.timing;if(c){var d=c.navigationStart,e=c.responseStart;if(e>d&&e<=window.start){window.start=e;b.wsrt=e-d;break a}}a.now&&(b.wsrt=Math.floor(a.now()))}}window.google=window.google||{};google.aft=function(f){f.setAttribute(\"data-iml\",+new Date)};}).call(this);(function(){window.jsarwt=function(){return!1};}).call(this);(function(){var c=[],e=0;window.ping=function(b){-1==b.indexOf(\"&zx\")&&(b+=\"&zx=\"+(new Date).getTime());var a=new Image,d=e++;c[d]=a;a.onerror=a.onload=a.onabort=function(){delete c[d]};a.src=b};}).call(this);</script><style>body{margin:0 auto;max-width:736px;padding:0 8px}a{color:#1967D2;text-decoration:none;tap-highlight-color:rgba(0,0,0,.1)}a:v'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZ-bnFDeHrjH"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Z2w7Q_AHrjH"
   },
   "source": [
    "### Exercise 2.\n",
    "\n",
    "1. Inspect the Google search results page and understand how results are displayed.\n",
    "\n",
    "\n",
    "2. Use BeautifulSoup to get the link of the first 10 results of this search out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4u2ydDrUHrjH"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Vertical Search Engine\n",
    "\n",
    "A *Vertical Search Engine* is a \"specialized\" search engine that focuses on a specific domain or service, tailored to the\n",
    "particular information needs of niche audiences and professions. These also have a more detailed API than the general Google search. So, we can use it to practice our API skills. But first, we have to make a Vertical Search Engine and get our API key.\n",
    "\n",
    "### Build the Vertical Search Engine\n",
    "\n",
    "We will use Google Co-op's custom search engine http://cse.google.com/. Follow these steps to create your search engine:\n",
    "\n",
    "* Click on ''New Search Engine'' and sign in.\n",
    "* Specify a list of websites to search. You can start with a few websites, and add more later (aim for 10-20). If you add too many, it may not search all of them --- you can see this in the Control Panel. Only if a website has a green checkmark, it will be searched.\n",
    "* Enter the basic information: language, name.\n",
    "* Click on ''Create''.\n",
    "\n",
    "To use your search engine in Python, you need two things: your Search Engine ID (visible in the CSE control panel) and an API key. To get a key, go to this page: https://developers.google.com/custom-search/v1/introduction and click Get a Key.\n",
    "\n",
    "Now, we can try to search using the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# get the API KEY here: https://developers.google.com/custom-search/v1/overview\n",
    "API_KEY = \"\"\n",
    "# get your Search Engine ID on your CSE control panel\n",
    "SEARCH_ENGINE_ID = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The query you want to search for.\n",
    "query = \"List of public APIs\"\n",
    "\n",
    "# using the first page\n",
    "page = 1\n",
    "\n",
    "# Making the link to Google to search\n",
    "# Documentation on this topic: https://developers.google.com/custom-search/v1/using_rest\n",
    "# Start should be the index of the first result you want to see, and each page has 10 results.\n",
    "# So if we want to see page 2, we need to start at result number 11.\n",
    "start = (page - 1) * 10 + 1\n",
    "# Building the link to send to Google\n",
    "url = f\"https://www.googleapis.com/customsearch/v1?key={API_KEY}&cx={SEARCH_ENGINE_ID}&q={query}&start={start}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: With the free version, you can only make 100 queries per day to Google (the requests.get part). So don't run the \"get\" queries too often or you will hit the limit before being able to finish the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the search request to the API. This is a cell you want to run as few times as possible.\n",
    "data = requests.get(url).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = data.get(\"items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a dictionary *data* containing the result of our request. The actual search results are stored in `data[\"items\"]` so we have saved that to the variable search_results. We can re-use this variable to avoid making many requests to Google.\n",
    "\n",
    "### Read through the search results\n",
    "\n",
    "Let's print the results in a nice way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_search_results(search_results):\n",
    "    for i, search_item in enumerate(search_results, start=1):\n",
    "        # get the page title\n",
    "        title = search_item.get(\"title\")\n",
    "        # page snippet\n",
    "        snippet = search_item.get(\"snippet\")\n",
    "        # alternatively, you can get the HTML snippet (bolded keywords)\n",
    "        html_snippet = search_item.get(\"htmlSnippet\")\n",
    "        # extract the page url\n",
    "        link = search_item.get(\"link\")\n",
    "        # print the results\n",
    "        print(\"=\"*15, f\"Result #{i+start-1}\", \"=\"*15)\n",
    "        print(\"Title:\", title)\n",
    "        print(\"Description:\", snippet)\n",
    "        print(\"URL:\", link, \"\\n\")\n",
    "        \n",
    "print_search_results(search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.\n",
    "\n",
    "1. We did not really cover the topic of Information Retrieval in this course, but we can of course use this to perform evaluation retrieval. But, how good is your vertical search engine? Try to make two different vertical search engines that target the same topic (for example, one with the \"Search the entire web\" setting enabled, and the other without), and evaluate them. Your gold standard should consist of a list of keywords and web pages that should be found using that keyword, and exist within the list of sites that your vertical search engine searches:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset_dict = {\n",
    "    \"huggingface api key\": \"https://huggingface.co/docs/hub/security-tokens\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code to compare the performance of your two vertical search engines using the Mean Reciprocal Rank metric (http://en.wikipedia.org/wiki/Mean_reciprocal_rank)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Also evaluate its Precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LyricsGenius API\n",
    "\n",
    "Another very nice API that has sometimes been used in projects for this course is the LyricsGenius API, allowing you to query the Genius website of song lyrics. They have their own Python package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lyricsgenius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, you need an API key, but it is free to make an account: https://genius.com/api-clients/new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lyricsgenius as lg\n",
    "api_key = \"\"\n",
    "genius = lg.Genius(api_key, skip_non_songs=True, excluded_terms=[\"(Remix)\", \"(Live)\"], remove_section_headers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can have a look at the documentation to see what you can do with this API, which is often what you have to do when working with APIs, as every API is different: https://lyricsgenius.readthedocs.io/en/master/usage.html\n",
    "\n",
    "For example, we can get an object that has all songs by an artist, and then print the lyrics of a song from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist = genius.search_artist(\"Rick Astley\")\n",
    "song = artist.song(\"Never Gonna Give You Up\")\n",
    "print(song.lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "1. Let's make an API pipeline. Use the LyricsGenius API to find the YouTube URL of a song that you are interested in, then use the YouTube Data API (https://developers.google.com/youtube/v3) to retrieve all comments made on the video for that song, and print the most liked one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Requests to web APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAgQPIS1HrjH"
   },
   "source": [
    "What about using `requests` to query APIs? Easy using the param dictionary. Responses then follow the standard format of the API (or you can request the one you like if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "huuD4tgfHrjH",
    "outputId": "be8ddac1-4ebd-4760-bbfa-de2decf61545"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'{\"current_user_url\":\"https://api.github.com/user\",\"current_user_authorizations_html_url\":\"https://github.com/settings/connections/applications{/client_id}\",\"authorizations_url\":\"https://api.github.com/authorizations\",\"code_search_url\":\"https://api.github.com/search/code?q={query}{&page,per_page,sort,order}\",\"commit_search_url\":\"https://api.github.com/search/commits?q={query}{&page,per_page,sort,order}\",\"emails_url\":\"https://api.github.com/user/emails\",\"emojis_url\":\"https://api.github.com/emojis\",\"events_url\":\"https://api.github.com/events\",\"feeds_url\":\"https://api.github.com/feeds\",\"followers_url\":\"https://api.github.com/user/followers\",\"following_url\":\"https://api.github.com/user/following{/target}\",\"gists_url\":\"https://api.github.com/gists{/gist_id}\",\"hub_url\":\"https://api.github.com/hub\",\"issue_search_url\":\"https://api.github.com/search/issues?q={query}{&page,per_page,sort,order}\",\"issues_url\":\"https://api.github.com/issues\",\"keys_url\":\"https://api.github.com/user/keys\",\"label_search_url\":\"https://api.github.com/search/labels?q={query}&repository_id={repository_id}{&page,per_page}\",\"notifications_url\":\"https://api.github.com/notifications\",\"organization_url\":\"https://api.github.com/orgs/{org}\",\"organization_repositories_url\":\"https://api.github.com/orgs/{org}/repos{?type,page,per_page,sort}\",\"organization_teams_url\":\"https://api.github.com/orgs/{org}/teams\",\"public_gists_url\":\"https://api.github.com/gists/public\",\"rate_limit_url\":\"https://api.github.com/rate_limit\",\"repository_url\":\"https://api.github.com/repos/{owner}/{repo}\",\"repository_search_url\":\"https://api.github.com/search/repositories?q={query}{&page,per_page,sort,order}\",\"current_user_repositories_url\":\"https://api.github.com/user/repos{?type,page,per_page,sort}\",\"starred_url\":\"https://api.github.com/user/starred{/owner}{/repo}\",\"starred_gists_url\":\"https://api.github.com/gists/starred\",\"topic_search_url\":\"https://api.github.com/search/topics?q={query}{&page,per_page}\",\"user_url\":\"https://api.github.com/users/{user}\",\"user_organizations_url\":\"https://api.github.com/user/orgs\",\"user_repositories_url\":\"https://api.github.com/users/{user}/repos{?type,page,per_page,sort}\",\"user_search_url\":\"https://api.github.com/search/users?q={query}{&page,per_page,sort,order}\"}'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get('https://api.github.com')\n",
    "\n",
    "# raw\n",
    "r.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QguxPZbHHrjI",
    "outputId": "82b9df6a-bc62-487d-ecce-adbf7456626c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'current_user_url': 'https://api.github.com/user',\n",
       " 'current_user_authorizations_html_url': 'https://github.com/settings/connections/applications{/client_id}',\n",
       " 'authorizations_url': 'https://api.github.com/authorizations',\n",
       " 'code_search_url': 'https://api.github.com/search/code?q={query}{&page,per_page,sort,order}',\n",
       " 'commit_search_url': 'https://api.github.com/search/commits?q={query}{&page,per_page,sort,order}',\n",
       " 'emails_url': 'https://api.github.com/user/emails',\n",
       " 'emojis_url': 'https://api.github.com/emojis',\n",
       " 'events_url': 'https://api.github.com/events',\n",
       " 'feeds_url': 'https://api.github.com/feeds',\n",
       " 'followers_url': 'https://api.github.com/user/followers',\n",
       " 'following_url': 'https://api.github.com/user/following{/target}',\n",
       " 'gists_url': 'https://api.github.com/gists{/gist_id}',\n",
       " 'hub_url': 'https://api.github.com/hub',\n",
       " 'issue_search_url': 'https://api.github.com/search/issues?q={query}{&page,per_page,sort,order}',\n",
       " 'issues_url': 'https://api.github.com/issues',\n",
       " 'keys_url': 'https://api.github.com/user/keys',\n",
       " 'label_search_url': 'https://api.github.com/search/labels?q={query}&repository_id={repository_id}{&page,per_page}',\n",
       " 'notifications_url': 'https://api.github.com/notifications',\n",
       " 'organization_url': 'https://api.github.com/orgs/{org}',\n",
       " 'organization_repositories_url': 'https://api.github.com/orgs/{org}/repos{?type,page,per_page,sort}',\n",
       " 'organization_teams_url': 'https://api.github.com/orgs/{org}/teams',\n",
       " 'public_gists_url': 'https://api.github.com/gists/public',\n",
       " 'rate_limit_url': 'https://api.github.com/rate_limit',\n",
       " 'repository_url': 'https://api.github.com/repos/{owner}/{repo}',\n",
       " 'repository_search_url': 'https://api.github.com/search/repositories?q={query}{&page,per_page,sort,order}',\n",
       " 'current_user_repositories_url': 'https://api.github.com/user/repos{?type,page,per_page,sort}',\n",
       " 'starred_url': 'https://api.github.com/user/starred{/owner}{/repo}',\n",
       " 'starred_gists_url': 'https://api.github.com/gists/starred',\n",
       " 'topic_search_url': 'https://api.github.com/search/topics?q={query}{&page,per_page}',\n",
       " 'user_url': 'https://api.github.com/users/{user}',\n",
       " 'user_organizations_url': 'https://api.github.com/user/orgs',\n",
       " 'user_repositories_url': 'https://api.github.com/users/{user}/repos{?type,page,per_page,sort}',\n",
       " 'user_search_url': 'https://api.github.com/search/users?q={query}{&page,per_page,sort,order}'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# json\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2Fsof5lHrjI"
   },
   "source": [
    "## Twitter API\n",
    "\n",
    "**Note**: Since Elon Musk's takeover of Twitter, this last section of the notebook has become less useful, as they now charge \\$100 a month even for basic access. Nevertheless, it is here for reference.\n",
    "\n",
    "Previous to that, developed their API version 2, which is unfortunately more limited than the old one, but you may still encounter both in code. The v1 version is no longer accessible even on the basic plan. This notebook uses the API v2. For a version using API v1, see last year's course materials.\n",
    "\n",
    "Two main APIs:\n",
    "\n",
    "* **Streaming API**: a sample of public tweets and events as they published on Twitter, provides only real-time data without limits.\n",
    "\n",
    "* **REST API**: allows to search, follow trends, read author profile and follower data, post / modify. It provides historical data up to a week (for the free account, more by paying), rwquires a one-time request and has rate limit (varies for different requests and subscriptions).\n",
    "\n",
    "\n",
    "REST APIs (it is a style for developing Web services which is widely used): https://en.wikipedia.org/wiki/Representational_state_transfer\n",
    "\n",
    "Some more basic info: https://developer.twitter.com/en/docs/basics/things-every-developer-should-know\n",
    "\n",
    "Tutorials: https://developer.twitter.com/en/docs/tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rElqtSZUHrjI"
   },
   "source": [
    "#### Using the API: authentication\n",
    "\n",
    "Let's do this on the Twitter dev website..\n",
    "\n",
    "A good way to store your keys is using `.conf` files and `configparser`. **DO NOT put them on GitHub.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RcsBg631HrjI",
    "outputId": "7c6bfff0-72ad-4b2f-f17c-475624fddd0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stuff/conf.conf']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser(interpolation=None)\n",
    "config.read(\"stuff/conf.conf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0N9YoHHpHrjI",
    "outputId": "8ebbf257-bb39-4ffa-c4ec-d46e53b4584d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hBn3fPoa7TXlL4fEEbZ5l1cbd'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['twitter']['api_key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WN4EFs0HrjJ"
   },
   "source": [
    "This is how my `conf.conf` file looks like (also in `stuff/conf_public.conf`):\n",
    "\n",
    "```\n",
    "[twitter]\n",
    "bearer_token = YOURS\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Z97YZLhHrjJ"
   },
   "source": [
    "#### A useful package: Tweepy\n",
    "\n",
    "https://tweepy.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5wefGqXG7Z3r"
   },
   "outputs": [],
   "source": [
    "#You will need a recent version of Tweepy to work with the new Twitter API\n",
    "#!pip install tweepy==4.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UdqXCQ3BHrjJ"
   },
   "outputs": [],
   "source": [
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-KUxZkNmHrjJ",
    "outputId": "53f3338f-bd88-4450-d32c-487196de1c93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @FMG_UvA: Orthopedagoog Levi van Dam is mede-initiatiefnemer van ‘We Spark the World’. Een campagne met challenges om #jongeren #mentaal…\n",
      "Drie UvA-wetenschapstalenten ontvangen een #Rubicon-subsidie van @NWONieuws! Gefeliciteerd 👏Laura Burgers @act_privatelaw, @TesselBouwens @UvA_Science en @SvanNeerven @amsterdamumc! Met de Rubicon kunnen wetenschappers ervaring op doen in het buitenland. https://t.co/P26jKaBkKV\n",
      "RT @FMG_UvA: Hoe #socialemedia onderdeel zijn geworden van de #oorlog.\n",
      "\n",
      "Interview met @UvA_Amsterdam communicatiewetenschappers @TomDobber…\n",
      "Door de oorlog in #Oekraïne is het humanitair oorlogsrecht de afgelopen weken veel in het nieuws geweest. De wisselwerking tussen het humanitair oorlogsrecht en het internationaal strafrecht zorgt voor uitdagingen, aldus promovendus Rogier Bartels (ICC): https://t.co/EVmCrWDjav\n",
      "RT @NKI_nl: We’re excited to officially launch our new AI-lab: POP-AART, together with @UvA_Amsterdam and @Elekta. Would you like to learn…\n"
     ]
    }
   ],
   "source": [
    "# Tweepy Hello World\n",
    "\n",
    "# authentication (OAuth 2.0)\n",
    "\n",
    "client = tweepy.Client(config['twitter']['bearer_token'])\n",
    "\n",
    "usertweets = client.get_users_tweets(id=156280168)\n",
    "\n",
    "for tweet in usertweets.data[:5]:\n",
    "    print(tweet.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haOiKUoxHrjJ"
   },
   "source": [
    "#### Interlude: JSON\n",
    "\n",
    "The Twitter API returns data structured in the JSON format. [JSON](https://www.json.org) (JavaScript Object Notation) is a lightweight data-interchange format. It is easy for humans to read and write. It is easy for machines to parse and generate. **It is basically a list of nested Python dictionaries.**\n",
    "\n",
    "\n",
    "Minimal example:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"firstName\": \"John\",\n",
    "  \"lastName\": \"Doe\",\n",
    "  \"age\": 21\n",
    "}\n",
    "```\n",
    "\n",
    "Extended example:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"$id\": \"https://example.com/person.schema.json\",\n",
    "  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "  \"title\": \"Person\",\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"firstName\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"The person's first name.\"\n",
    "    },\n",
    "    \"lastName\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"The person's last name.\"\n",
    "    },\n",
    "    \"age\": {\n",
    "      \"description\": \"Age in years which must be equal to or greater than zero.\",\n",
    "      \"type\": \"integer\",\n",
    "      \"minimum\": 0\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "Online viewer: http://jsonviewer.stack.hu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qkq2kJ_FHrjJ"
   },
   "source": [
    "#### Using the API: search\n",
    "\n",
    "All the most recent Tweets from a given hashtag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mwq_hB8AHrjK",
    "outputId": "bed023bd-8af6-4308-cfb6-782ae6bff800",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1514575939883216902', 'text': 'RT @KirkDBorne: Use *already solved* #DataScience projects for your real-world business problems now. Explore many examples here: https://t…'}\n",
      "RT @KirkDBorne: Use *already solved* #DataScience projects for your real-world business problems now. Explore many examples here: https://t…\n",
      "{'id': '1514575718361051143', 'text': 'RT @SapienzaNLP: #NLPaperAlert 📢 We bring together existing resources, revise them, and propose SRL4E, a unified evaluation on Semantic Rol…'}\n",
      "RT @SapienzaNLP: #NLPaperAlert 📢 We bring together existing resources, revise them, and propose SRL4E, a unified evaluation on Semantic Rol…\n",
      "{'id': '1514574359582720013', 'text': 'RT @gp_pulipaka: Clinical Named Entity Recognition Using spaCy! #BigData #Analytics #DataScience #AI #MachineLearning #NLProc #IoT #IIoT #P…'}\n",
      "RT @gp_pulipaka: Clinical Named Entity Recognition Using spaCy! #BigData #Analytics #DataScience #AI #MachineLearning #NLProc #IoT #IIoT #P…\n",
      "{'id': '1514569941994942473', 'text': 'RT @lajello: 🚨 Job openings in Copenhagen! 🚨3 PhD/postdoc positions in the @nerdsitu team at @ITUkbh on my COCOONS project on collective co…'}\n",
      "RT @lajello: 🚨 Job openings in Copenhagen! 🚨3 PhD/postdoc positions in the @nerdsitu team at @ITUkbh on my COCOONS project on collective co…\n",
      "{'id': '1514568620252319745', 'text': 'RT @SapienzaNLP: #NLPaperAlert 📢 We bring together existing resources, revise them, and propose SRL4E, a unified evaluation on Semantic Rol…'}\n",
      "RT @SapienzaNLP: #NLPaperAlert 📢 We bring together existing resources, revise them, and propose SRL4E, a unified evaluation on Semantic Rol…\n",
      "{'id': '1514568135323828233', 'text': 'RT @wittgen_ball: #NLProc folks: Beam search with controlled patience improves text generation.\\nJust change a 𝐬𝐢𝐧𝐠𝐥𝐞 𝐥𝐢𝐧𝐞 in your codebase!…'}\n",
      "RT @wittgen_ball: #NLProc folks: Beam search with controlled patience improves text generation.\n",
      "Just change a 𝐬𝐢𝐧𝐠𝐥𝐞 𝐥𝐢𝐧𝐞 in your codebase!…\n",
      "{'id': '1514566996108660737', 'text': 'RT @KirkDBorne: Use *already solved* #DataScience projects for your real-world business problems now. Explore many examples here: https://t…'}\n",
      "RT @KirkDBorne: Use *already solved* #DataScience projects for your real-world business problems now. Explore many examples here: https://t…\n",
      "{'id': '1514566656009322503', 'text': 'In his #ECIR2022 Industry Day talk (15:30 CEST), \"The Impact and Importance of Keyphrases in Building NLP Products,\" #AI Researcher Mayank Kulkarni will discuss the roles of Keyphrase Extraction &amp; Keyphrase Generation in the context of #NLProc in finance\\nhttps://t.co/T0aA0S4A93'}\n",
      "In his #ECIR2022 Industry Day talk (15:30 CEST), \"The Impact and Importance of Keyphrases in Building NLP Products,\" #AI Researcher Mayank Kulkarni will discuss the roles of Keyphrase Extraction &amp; Keyphrase Generation in the context of #NLProc in finance\n",
      "https://t.co/T0aA0S4A93\n",
      "{'id': '1514564915675250695', 'text': '#NLPaperAlert 📢 We bring together existing resources, revise them, and propose SRL4E, a unified evaluation on Semantic Role Labeling 4 Emotions!\\n\\nRead our #ACL2022 preprint: https://t.co/Wnqe4waMUD\\n\\nBy @caesar_one_ @ConiaSimone @RNavigli\\n+ @ERC_Research @EuroLangTech #NLProc https://t.co/WXzAzFJZa3'}\n",
      "#NLPaperAlert 📢 We bring together existing resources, revise them, and propose SRL4E, a unified evaluation on Semantic Role Labeling 4 Emotions!\n",
      "\n",
      "Read our #ACL2022 preprint: https://t.co/Wnqe4waMUD\n",
      "\n",
      "By @caesar_one_ @ConiaSimone @RNavigli\n",
      "+ @ERC_Research @EuroLangTech #NLProc https://t.co/WXzAzFJZa3\n",
      "{'id': '1514564591661076482', 'text': 'RT @JagersbergKnut: MetaVec: The Best Word Embeddings To Date\\n\\nDownload pre-computed meta-embedding. This meta embeddings combines FastText…'}\n",
      "RT @JagersbergKnut: MetaVec: The Best Word Embeddings To Date\n",
      "\n",
      "Download pre-computed meta-embedding. This meta embeddings combines FastText…\n",
      "{'id': '1514563560172658699', 'text': \"RT @TechAtBloomberg: Stop by today's #ECIR2022 Industry Day poster session (13:30 CEST) to talk with #AI Researcher Mayank Kulkarni about t…\"}\n",
      "RT @TechAtBloomberg: Stop by today's #ECIR2022 Industry Day poster session (13:30 CEST) to talk with #AI Researcher Mayank Kulkarni about t…\n",
      "{'id': '1514561790780260352', 'text': \"RT @TechAtBloomberg: Stop by today's #ECIR2022 Industry Day poster session (13:30 CEST) to talk with #AI Researcher Mayank Kulkarni about t…\"}\n",
      "RT @TechAtBloomberg: Stop by today's #ECIR2022 Industry Day poster session (13:30 CEST) to talk with #AI Researcher Mayank Kulkarni about t…\n",
      "{'id': '1514559204241338374', 'text': 'RT @gp_pulipaka: Pay $18 for These 15 Items. #BigData #Analytics #DataScience #IoT #IIoT #PyTorch #Python #RStats #TensorFlow #NLProc #Java…'}\n",
      "RT @gp_pulipaka: Pay $18 for These 15 Items. #BigData #Analytics #DataScience #IoT #IIoT #PyTorch #Python #RStats #TensorFlow #NLProc #Java…\n",
      "{'id': '1514555373277319170', 'text': 'RT @KirkDBorne: Luv this! 👉 Top 50 #MachineLearning Projects Ideas for Beginners in 2022:\\n👉https://t.co/jy4dXu9Tgg by https://t.co/z4oRjqIs…'}\n",
      "RT @KirkDBorne: Luv this! 👉 Top 50 #MachineLearning Projects Ideas for Beginners in 2022:\n",
      "👉https://t.co/jy4dXu9Tgg by https://t.co/z4oRjqIs…\n",
      "{'id': '1514555334374866946', 'text': \"Stop by today's #ECIR2022 Industry Day poster session (13:30 CEST) to talk with #AI Researcher Mayank Kulkarni about the roles of two key #NLProc tasks -- Keyphrase Extraction &amp; Keyphrase Generation -- in the context of NLP in the financial domain\\nhttps://t.co/mmrWJ93m4I https://t.co/uKALJreyHR\"}\n",
      "Stop by today's #ECIR2022 Industry Day poster session (13:30 CEST) to talk with #AI Researcher Mayank Kulkarni about the roles of two key #NLProc tasks -- Keyphrase Extraction &amp; Keyphrase Generation -- in the context of NLP in the financial domain\n",
      "https://t.co/mmrWJ93m4I https://t.co/uKALJreyHR\n",
      "{'id': '1514551070273515523', 'text': 'RT @wittgen_ball: #NLProc folks: Beam search with controlled patience improves text generation.\\nJust change a 𝐬𝐢𝐧𝐠𝐥𝐞 𝐥𝐢𝐧𝐞 in your codebase!…'}\n",
      "RT @wittgen_ball: #NLProc folks: Beam search with controlled patience improves text generation.\n",
      "Just change a 𝐬𝐢𝐧𝐠𝐥𝐞 𝐥𝐢𝐧𝐞 in your codebase!…\n",
      "{'id': '1514550290384572420', 'text': 'RT @davidschlangen: There is a lot of talk on my timeline of #NLProc / #AI walls being hit and/or smashed,  goalposts being missed and/or m…'}\n",
      "RT @davidschlangen: There is a lot of talk on my timeline of #NLProc / #AI walls being hit and/or smashed,  goalposts being missed and/or m…\n",
      "{'id': '1514547668709740549', 'text': 'RT @TechAtBloomberg: (2/5) @Daniel_Preotiuc and the @SheffieldNLP team of Xiao Ao, @danaesavi &amp; @NikAletras had their paper “Combining Humo…'}\n",
      "RT @TechAtBloomberg: (2/5) @Daniel_Preotiuc and the @SheffieldNLP team of Xiao Ao, @danaesavi &amp; @NikAletras had their paper “Combining Humo…\n",
      "{'id': '1514543460933636099', 'text': 'RT @lajello: 🚨 Job openings in Copenhagen! 🚨3 PhD/postdoc positions in the @nerdsitu team at @ITUkbh on my COCOONS project on collective co…'}\n",
      "RT @lajello: 🚨 Job openings in Copenhagen! 🚨3 PhD/postdoc positions in the @nerdsitu team at @ITUkbh on my COCOONS project on collective co…\n",
      "{'id': '1514541771912806408', 'text': 'RT @wittgen_ball: #NLProc folks: Beam search with controlled patience improves text generation.\\nJust change a 𝐬𝐢𝐧𝐠𝐥𝐞 𝐥𝐢𝐧𝐞 in your codebase!…'}\n",
      "RT @wittgen_ball: #NLProc folks: Beam search with controlled patience improves text generation.\n",
      "Just change a 𝐬𝐢𝐧𝐠𝐥𝐞 𝐥𝐢𝐧𝐞 in your codebase!…\n"
     ]
    }
   ],
   "source": [
    "# queries\n",
    "\n",
    "for tweet in tweepy.Paginator(client.search_recent_tweets, \"#nlproc\", max_results=10).flatten(limit=20):\n",
    "    print(tweet.data)\n",
    "    print(tweet.data[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cN6yJpTbHrjK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# queries with more information fields\n",
    "import json\n",
    "\n",
    "for tweet in tweepy.Paginator(client.search_recent_tweets, \"#nlproc\", max_results=10, \n",
    "                              tweet_fields=[\"created_at\", \"in_reply_to_user_id\", \"referenced_tweets\", \"public_metrics\"]).flatten(limit=5):\n",
    "    print(json.dumps(tweet.data, indent=4, sort_keys=False))\n",
    "#for item in tweets.items(5):\n",
    "#    print(json.dumps(item._json, indent=4, sort_keys=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vW4OfUDEHrjK"
   },
   "source": [
    "#### Using the API: users\n",
    "\n",
    "Get some info on a given user, and explore their friends/followers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NH8cVOf2HrjK",
    "outputId": "4a054910-4520-4c87-94e8-fd46ecc7c9d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(data=<User id=44196397 name=Elon Musk username=elonmusk>, includes={}, errors=[], meta={})\n",
      "User: elonmusk\n",
      "------\n",
      "Following: 113\n",
      "Followers: 81656079\n",
      "------\n",
      "Following:\n",
      "beeple\n",
      "Grimezsz\n",
      "thesheetztweetz\n",
      "EvaFoxU\n",
      "planet\n",
      "Teslarati\n",
      "OfficialPCMR\n",
      "stats_feed\n",
      "universal_sci\n",
      "gunsnrosesgirl3\n",
      "------\n",
      "Followers:\n",
      "Gamewiz18\n",
      "Miguelib88\n",
      "SunbridgeConsu1\n",
      "Barcaismyclub\n",
      "PetrasPhyllis\n",
      "mdreichard3\n",
      "ZeusZeu05906203\n",
      "cryptoburrenci\n",
      "han54282601\n",
      "BrigSpurck\n"
     ]
    }
   ],
   "source": [
    "user = client.get_user(username=\"elonmusk\", user_fields=[\"public_metrics\"])\n",
    "print(user)\n",
    "print(\"User:\",user.data.username)\n",
    "print(\"------\")\n",
    "print(\"Following:\",user.data.public_metrics[\"following_count\"])\n",
    "print(\"Followers:\",user.data.public_metrics[\"followers_count\"])\n",
    "print(\"------\")\n",
    "print(\"Following:\")\n",
    "friends = client.get_users_following(user.data.id, max_results=10)[0]\n",
    "for friend in friends:\n",
    "    print(friend.data[\"username\"])\n",
    "print(\"------\")\n",
    "print(\"Followers:\")\n",
    "friends = client.get_users_followers(user.data.id, max_results=10)[0]\n",
    "for friend in friends:\n",
    "    print(friend.data[\"username\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2Mnw-y2HrjL"
   },
   "source": [
    "#### Using the API: tweets from user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-qz-7IImHrjL",
    "outputId": "90b73c07-dccc-4e74-fa61-38a7920331f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I made an offer \n",
      "https://t.co/VvreuPMeLu\n",
      "RT @SpaceX: Photos from the @space_station of Dragon and the Ax-1 astronauts approaching the orbiting laboratory before docking on Saturday…\n",
      "@JeffBezos Great idea\n",
      "@BLKMDL3 Coming soon\n",
      "@tesla_raj Exciting times ahead\n"
     ]
    }
   ],
   "source": [
    "#user = api.get_user(\"elonmusk\", tweet_mode=\"extended\") # extended tweetmode gets also the longer 280/char tweets\n",
    "#elon_tweets = user.timeline()\n",
    "\n",
    "#for tweet in elon_tweets[:5]:\n",
    "#    print(tweet.text)\n",
    "\n",
    "usertweets = client.get_users_tweets(id=client.get_user(username=\"elonmusk\").data.id)\n",
    "\n",
    "for tweet in usertweets.data[:5]:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIVePFrdHrjL"
   },
   "source": [
    "#### Twitter data: tokenizing\n",
    "\n",
    "Tokenizing tweets requires a dedicated apporoach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "euZ5YejIHrjL"
   },
   "outputs": [],
   "source": [
    "a_tweet = \"@moyo5150 More like https://t.co/f8BUlz0Xdo #robots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jrSgyimgHrjL",
    "outputId": "d7454efd-d9b2-4f69-d277-78959124a88e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " 'moyo5150',\n",
       " 'More',\n",
       " 'like',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/f8BUlz0Xdo',\n",
       " '#',\n",
       " 'robots']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('punkt')\n",
    "\n",
    "word_tokenize(a_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ku9Z32TZHrjM",
    "outputId": "7eaa5652-b74a-410a-8791-815e0bb7e7d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@moyo5150', 'More', 'like', 'https://t.co/f8BUlz0Xdo', '#robots']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer(preserve_case=True, reduce_len=False, strip_handles=False)\n",
    "tweet_tokenizer.tokenize(a_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXH4W_S_HrjM"
   },
   "source": [
    "Other options are available, e.g., the [ark-twokenizer](https://github.com/myleott/ark-twokenize-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEKdCgdRHrjM"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKFyiCojHrjM"
   },
   "source": [
    "For those who don't have a Twitter account and app, here are some tweets on and by Boris Johnson!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AmUUdTt1HrjM"
   },
   "outputs": [],
   "source": [
    "user = \"@BorisJohnson\"\n",
    "#tweets_on_user = tweepy.Cursor(api.search, q=user, tweet_mode=\"extended\")\n",
    "tweets_on_user = tweepy.Paginator(client.search_recent_tweets, user, max_results=100).flatten(limit=1000)\n",
    "\n",
    "on_boris = list()\n",
    "for tweet in tweets_on_user:\n",
    "    on_boris.append(tweet.data[\"text\"])\n",
    "    \n",
    "#print(\"\\n------\\n\")\n",
    "# from user\n",
    "\n",
    "#user = api.get_user(\"BorisJohnson\", tweet_mode=\"extended\") # extended tweetmode gets also the longer 280/char tweets\n",
    "#tweets_from_user = user.timeline(count=100)\n",
    "\n",
    "usertweets = tweepy.Paginator(client.get_users_tweets, id=client.get_user(username=\"BorisJohnson\").data.id, max_results=100).flatten(limit=1000)\n",
    "\n",
    "from_boris = list()\n",
    "for tweet in usertweets:\n",
    "    from_boris.append(tweet.data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KDDp5DF1HrjN"
   },
   "outputs": [],
   "source": [
    "# save to file\n",
    "f_on_boris = \"stuff/tweets_on_boris.csv\"\n",
    "f_from_boris = \"stuff/tweets_from_boris.csv\"\n",
    "\n",
    "# note we are using the \"\" as text delimiter\n",
    "with open(f_on_boris, \"w\") as f:\n",
    "    for t in on_boris:\n",
    "        f.write('\"'+t+'\"\\n')\n",
    "        \n",
    "with open(f_from_boris, \"w\") as f:\n",
    "    for t in from_boris:\n",
    "        f.write('\"'+t+'\"\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K48W7NQIHrjN"
   },
   "source": [
    "### Exercise 5 (if you are rich):\n",
    "\n",
    "1. Download the last 100 (or another number) tweets mentioning a user you are interested into and the last 100 from the user itself. Alternatively, use the tweets in the on_boris and from_boris files.\n",
    "\n",
    "\n",
    "2. Create a minimal pipeline to normalize the tweets into lists of tokens.\n",
    "\n",
    "\n",
    "3. Count and compare from the two datasets, the most frequent (top 10):\n",
    "    - tokens\n",
    "    - hashtags\n",
    "    - other user mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E4qLI-m4HrjN"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "10_WebScraping_APIs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
